{"expireTime":9007200845577683000,"key":"transformer-remark-markdown-html-15b657ad0155771c9b1d12383bda9986-gatsby-plugin-sharpgatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-emojis-","val":"<p>100万×100万の行列を作って、機械学習をぶん回して…とやっていると\n本当に心配になるくらいmacが熱くなりますし、メモリも食って、計算終わらなくて。。。\nということでスパース表現ですね。</p>\n<h2>スパース表現とは</h2>\n<p>行列の要素のほとんどが0、わずかにしか値が入ってないようなとき、\n例えばこんなん</p>\n<pre class=\"lang:python decode:true \" title=\"matrix\">array([[4, 0, 9, 0],\n       [0, 7, 0, 0],\n       [0, 0, 0, 0],\n       [0, 0, 0, 5]])</pre>\n<p>０をわざわざデータとして保持するんじゃなくて、\nわずかに入る値とその位置だけ保持して、あとは0であるとみなすことで\n持っているべき情報量を落とすための表現です。\nつまり、上の行列は</p>\n<pre class=\"lang:python decode:true \" title=\"matrix\">row  = np.array([0, 3, 1, 0])\ncol  = np.array([0, 3, 1, 2])\ndata = np.array([4, 5, 7, 9])\n</pre>\n<p>みたいに表せます。rowとcolがデータの位置、dataがそこに入る値になってます。\nこれを</p>\n<pre class=\"lang:python decode:true \" title=\"matrix\">sparse_matrix = coo_matrix((data, (row, col)), shape=(4, 4))\n</pre>\n<p>というようにしてスパース行列にします。<a href=\"https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.sparse.coo_matrix.html#scipy.sparse.coo_matrix\">参考：公式</a>\n元の、全ての値を保持する表現はスパースに対してデンス(dense)と呼ばれます。\n<a href=\"http://qiita.com/kazk1018/items/c338b2883b4a58298bcf\">こちらの記事</a>が詳しい。</p>\n<h2>denseの扱いとの違い</h2>\n<p>基本的にはdenseの場合と同様の処理ができます。\nただ、一般的な結合などもスパース行列用に関数がありますのでそちらを利用します。<a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.vstack.html\">例：縦方向の結合</a></p>\n<h2>scikit-learnと一緒に使う</h2>\n<p>scikit-learnの多くのアルゴリズムはスパース行列に対応してるので、\n今まで普通に行列を引数で渡していたように引数に渡せます。\n例えば<a href=\"http://www.kamishima.net/archive/recsys.pdf\">推薦システム</a>などに使う<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html\">非負値行列因子分解(Non-negative Matrix Factorization)</a>のParameterの項を見ると</p>\n<blockquote>\n<p>X: {array-like, sparse matrix}, shape (n<em>samples, n</em>features) : Data matrix to be decomposed</p>\n</blockquote>\n<p>となってるので、sparse matrixを引数に取れるよ、ということですね。\nNMFについては<a href=\"http://qiita.com/takechanman/items/6d1f65f94f7aaa016377\">こちら</a>がわかりやすい。\n実例はこんな感じ　<a href=\"http://www.benfrederickson.com/matrix-factorization/\">http://www.benfrederickson.com/matrix-factorization/</a></p>\n<h2>注意事項</h2>\n<p>拙作よりもこちらの記事を読んだ方がよいですが\n<a href=\"http://hamukazu.com/2014/01/30/sparse-vector-with-scipy-sparse/\">http://hamukazu.com/2014/01/30/sparse-vector-with-scipy-sparse/</a></p>\n<p>使うためだけの説明をすると、COOは計算に向かないので、CSRかCSCにします。\nCSRは行方向、CSCは列方向の計算を高速化するので、行列の形に応じて使い分けしましょう。</p>\n<h2>サンプルコード</h2>\n<p>例えばさきほどのNMFでは</p>\n<pre class=\"lang:python decode:true \" title=\"matrix\">import pandas as pd\nimport numpy as np\nimport scipy\nfrom sklearn.decomposition import NMF\n\ndef feature_extraction(mat_coo, num_features):\n    X = mat_coo.tocsr()\n    model = NMF(n_components=num_features, init='random', random_state=0)\n    model.fit(X)\n    print('feature extraction done')\n    return model.components_\n\nif __name__ == '__main__':\n\n    num_features = 3\n    print('now loading...')\n    row  = np.array([0, 3, 1, 0])\n    col  = np.array([0, 3, 1, 2])\n    data = np.array([4, 5, 7, 9])\n    mat_coo = scipy.sparse.coo_matrix((data, (row, col)), shape=(4, 4))\n\n    features = feature_extraction(mat_coo, num_features)\n\n</pre>\n<p>みたいな感じでできます。</p>\n<p>圧倒的に高速なので、データがスパースな場合にはぜひ活用しましょう。</p>"}