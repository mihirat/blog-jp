{"expireTime":9007200845574950000,"key":"transformer-remark-markdown-html-fece29baba91347c932cea9706e9c348-gatsby-plugin-sharpgatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-emojis-","val":"<p>社内でIstioを導入した際に、チュートリアルなどではわかりにくいハマりポイントや、既存GKEクラスタの導入にあたって留意すべき点がけっこうあるなと思ったのでまとめます。</p>\n<p>対象読者は</p>\n<ul>\n<li>GKEですでにサービスなどを運用している</li>\n<li>Istioの概要は知っている、またはチュートリアルは終わった</li>\n</ul>\n<p>くらいの方を想定しています。</p>\n<p>なお、Istioとはなにかを解説する記事はすでにたくさんあるので、できるだけ作業レベルで導入の仕方を解説していきます。\nIstioを知るハンズオンとしては、Googleの中の方の<a href=\"https://medium.com/google-cloud-jp/istio-1-0-%E3%82%92%E8%A9%A6%E3%81%97%E3%81%A6%E3%81%BF%E3%81%9F-d74f75eeb1b1\">Istio 1.0 を試してみた！</a>などが良いです。</p>\n<h2>移行したいアプリの構成</h2>\n<p>もともとの構成はこんな感じでした。</p>\n<ul>\n<li>GCP上でglobal IPを取得し、FQDNと紐づけて外部流入をさせる</li>\n<li>トラフィックはIngressで受けており、SSL terminationを行う</li>\n<li>Ingressの次にNginxがあり、バックエンドのpodへkube-dnsを用いてリバースプロキシさせる</li>\n<li>バックエンドのpodでは、GCSにアクセスしてリソースを取得するなどの処理をしつつレスポンスを返すサーバーが動作</li>\n</ul>\n<p>なお、下記の手順は、クラスタを別で新しく作成して、そこにIstioを導入する想定で進めます。</p>\n<p>既存クラスタ上で作業するとダウンタイムが出る危険性が高い気がします。\n（namespaceを既存と別に切ればいいような気もしますが、既存namespaceに影響が出ないかを自分は確認していません）</p>\n<h2>やることの流れ</h2>\n<p>1.　クラスタ作成\n2.　クラスタへのIstio導入と、namespaceへの有効化\n3.　namespace内へのリソースのデプロイ</p>\n<h2>準備</h2>\n<p>本家のチュートリアルに沿って、Istioを導入します。\n利用するのは</p>\n<p><code class=\"language-text\">$ kubectl apply -f install/kubernetes/istio-demo.yaml</code></p>\n<p>です。\nistio-demo-auth.yamlでは、mutualTLSといってpod間の通信が全てTLSになって最高！に見えるのですが、readiness probeとliveness probeが使えなくなります\n<a href=\"https://github.com/istio/istio/issues/2628\">サービスが動いているportとは別にprobe専用のportを空けるという話</a>もありますが、それってReadinessの意味がないような？</p>\n<p>無事にIstioが導入できたら（サンプルアプリのデプロイまで行ってしっかり確認しましょう）、既存アプリをデプロイするためのnamespaceを作成します。</p>\n<p>それが終わったら、下記コマンドでnamespace全体に対してIstioを有効化します。</p>\n<p><code class=\"language-text\">$　kubectl label namespace my-app istio-injection=enabled</code></p>\n<p>この操作によって、このnamespace内で以後作成されるpodは全て、\nIstioで利用されているEnvoyをsidecarとして自動的に注入された状態で立ち上がることになります。\nこの機能はkubernetes1.9以上じゃないと使えません（kubernetesのpod initializerを利用しているため）</p>\n<h2>知るべきこと1. IstioではIngressは使わない</h2>\n<p>最初にして最大の難関。\nIstioではIngressは使わず、Istio独自のGatewayとVirtualServiceというリソースを用います。（後述）</p>\n<p>(0.8からこうなったらしく、２０１７年の記事を読むと混乱します。後方互換性のためか公式にもまだIngress関係のリソースが残っていますが、使わないべきです。<a href=\"https://github.com/istio/istio/issues/1024\">参考</a>)</p>\n<p>その理由としては、IngressではIstioのもつ機能が全部活用できない、とのこと。</p>\n<blockquote>\n<p>In a Kubernetes environment, the Kubernetes Ingress Resource is used to specify services that should be exposed outside the cluster. In an Istio service mesh, a better approach (which also works in both Kubernetes and other environments) is to use a different configuration model, namely Istio Gateway. A Gateway allows Istio features such as monitoring and route rules to be applied to traffic entering the cluster.</p>\n</blockquote>\n<p><a href=\"https://istio.io/docs/tasks/traffic-management/ingress/#determining-the-ingress-ip-and-ports-when-using-a-node-port\">本家より</a></p>\n<p>で、そのアオリというわけではないんですが、GCPのglobal IPがIstioでは使えません。しばらくはサポートする予定もないみたいです。</p>\n<p>暫定では、ServiceのLoadbalancerIPを用いてくれ、という回答。</p>\n<blockquote>\n<p>Per our chat, it is possible to set the load balancer IP in a LoadBalancer service (search for loadBalancerIP on <a href=\"https://kubernetes.io/docs/concepts/services-networking/service/\">https://kubernetes.io/docs/concepts/services-networking/service/</a>). This can be set to a regional static IP but not global static IP - this is limited in Arcus and there’s no plan to support global. The missing part is that you have to use the actual IP address instead of a nice label like you can with an Ingress (kubernetes.io/ingress.global-static-ip-name: my-static-ip).</p>\n</blockquote>\n<p><a href=\"https://github.com/istio/istio/issues/3800\">Issue</a></p>\n<p>なので、Ingressで行っていた下記のような書き方はできません。SSL　terminationも別のところでやる必要があります。</p>\n<pre class=\"lang:yaml decode:true \">apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: my-app\n  annotations:\n    kubernetes.io/ingress.global-static-ip-name: \"gad-my-app\" # 使えない\n    kubernetes.io/ingress.allow-http: \"false\"\n    ingress.gcp.kubernetes.io/pre-shared-cert: \"myapp-cert\"　# 使えない\n</pre>\n<p>ということで、Ingressで構成を作ってしまっていた場合、IPアドレスの受け先を変更する必要があるため、blue-greenデプロイで既存GKEアプリを移行することになるかと思います。</p>\n<p>以下、これを実行していきます。</p>\n<h3>Regional IPを使って、IstioのServiceのLoadBalancerIPにpatchを当てる</h3>\n<p>k8sのServiceでは、LoadBalancerIPを既存IPに張り替えることで、そのServiceでそのIPアドレスのトラフィックを受けることができます。</p>\n<p>Istioをクラスタに導入すると、istio-systemというnamespaceにistio-ingressgatewayというk8sのServiceリソースが作成されます。</p>\n<p>Istioを導入したクラスタ１つにつき静的IPアドレスを一つ用意して、そのIPをistio-ingressgatewayに割り当てます。\n具体的には</p>\n<blockquote>\n<p>$ kubectl patch svc istio-ingressgateway —namespace istio-system \\ —patch ’{“spec”: { “loadBalancerIP”: ”<your-reserved-static-ip>” }’</your-reserved-static-ip></p>\n</blockquote>\n<p>です。<a href=\"https://github.com/knative/docs/blob/master/serving/gke-assigning-static-ip-address.md\">KnativeのReadme</a>を参考にしました。</p>\n<p>これで、IPアドレス宛に来たトラフィックは全てIstioのLBを通って来ます。</p>\n<h3>SSL　termination周りのリソースをsecretとしてデプロイ</h3>\n<p>Ingressで実行していたterminationは、前述のGatewayで行うことになります。\n流れとしては、まずsecretをデプロイします。</p>\n<pre class=\"lang:yaml decode:true \">apiVersion: v1\nkind: Secret\nmetadata:\n  name: istio-ingressgateway-certs # 予約語\n  namespace: istio-system\ntype: Opaque\ndata:\n  tls.key: my-key-base64\n  tls.crt: my-crt-base64\n</pre>\n<p>これで証明書などをIstioのnamespaceにデプロイして、Gatewayから呼び出します。\nGatewayは下記のように記述して、 <code class=\"language-text\">$ kubectl apply -f gateway.yaml</code>　でデプロイします。</p>\n<pre class=\"lang:yaml decode:true \">apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: my-gateway\n  namespace: my-app\nspec:\n  selector:\n    istio: ingressgateway # use istio default controller\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      serverCertificate: /etc/istio/ingressgateway-certs/tls.crt # 合わせる\n      privateKey: /etc/istio/ingressgateway-certs/tls.key # 合わせる\n    hosts:\n    - dev.my-app.com\n</pre>\n<p>このあたりは<a href=\"https://istio.io/docs/tasks/traffic-management/secure-ingress/#configure-a-tls-ingress-gateway\">本家を参照</a>すると出てきます。</p>\n<p>Gatewayはあくまでもトラフィックを受けるためだけのリソースで、受けたリソースをどこに流すかはVirtualServiceが担当します。</p>\n<pre class=\"lang:yaml decode:true \">apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: my-virtualservice\n  namespace: my-app\nspec:\n  hosts:\n  - dev.my-app.com\n  gateways:\n  - my-gateway\n  http:\n  - route:\n    - destination:\n        host:\n         my-nginx # my-nginxのk8s Serviceに対してトラフィックを流す\n        port:\n          number: 80\n</pre>\n<h2>知るべきこと2. Istio内部ではHTTP/1.0は利用できない</h2>\n<p>「まだ1.0とかありえないしｗ」と思われる方もいるかもしれませんが、Nginxのproxy_passのデフォルト設定では1.0になっています。\n1.0のままだと、Nginxからの疎通自体はできるものの、Statusコードが426になって返ってきます。\n<a href=\"https://github.com/envoyproxy/envoy/issues/2506\">該当Issue</a></p>\n<p>Istio公式のQuickstartより。</p>\n<blockquote>\n<p>Note: The application must use HTTP/1.1 or HTTP/2.0 protocol for all its HTTP traffic because HTTP/1.0 is not supported.</p>\n</blockquote>\n<p>なので、Nginxの場合は</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">proxy_http_version 1.1;</code></pre></div>\n<p>などを定義してやる必要があります。</p>\n<p>なお、kube-dnsが適切に動作していれば、</p>\n<p><code class=\"language-text\">proxy_pass http://&lt;service-name&gt;.&lt;cluster-namespace&gt;.svc.cluster.local</code></p>\n<p>などは引き続きそのまま利用できます。\n<a href=\"https://github.com/istio/istio/blob/master/samples/bookinfo/src/productpage/productpage.py#L54\">本家チュートリアル内でデプロイされるサンプルアプリ</a>も参考になります。</p>\n<h2>知るべきこと3. APIなど外部通信は全てHostnameなどで穴あけが必要</h2>\n<p>個人的に一番詰まったのはここでした。\nIstioでは、通信する外部ホストの全てをリストアップして記述してやる必要があります。\n例えば、<code class=\"language-text\">pip install</code>をする場合は<code class=\"language-text\">pypi.org</code>、google apiを利用する場合は<code class=\"language-text\">www.googleapi.com</code>など。\n詳細は<a href=\"https://istio.io/blog/2018/egress-https/\">こちらの本家記事</a>を読まれることをおすすめします。</p>\n<p>今回想定しているアプリでは、Google Cloud Storageと通信するので、\n利用するAPIのホストとプロトコルは下記の２つ。</p>\n<pre class=\"lang:yaml decode:true \">https://storage.googleapi.com\nhttp://metadata.google.internal\n</pre>\n<p>しかしながら、metadata.google.internalには罠があり、生IPを記述しないとcloud storage apiが使えないという問題が。\n<a href=\"https://github.com/istio/istio/issues/5288\">該当Issue</a></p>\n<p>一方で、生IPではなくA recordを要求する場合もあるようです。\n自分は GKEのVMのdefault credentialsを取得する際に、そこで</p>\n<blockquote>\n<p>503 Failed to retrieve <a href=\"http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true\">http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true</a></p>\n</blockquote>\n<p>のようなエラーが表示されてしまったのですが、これはFQDNを併記することで解決しました。\nこれらを用いて、下記のように記述して<code class=\"language-text\">kubectl apply -f external.yaml</code>　などでデプロイします。</p>\n<pre class=\"lang:yaml decode:true \">apiVersion: networking.istio.io/v1alpha3\nkind: ServiceEntry # Egress相当の役割をもつリソース\nmetadata:\n  name: external-google-api\n  namespace: my-app\nspec:\n  hosts:\n  - \"*.googleapis.com\" # wildcardが使える\n  - 169.254.169.254 # metadata.google.internalのIPアドレス\n  - \"metadata.google.internal\" # 併記\n  location: MESH_EXTERNAL\n  ports:\n  - number: 443\n    name: https\n    protocol: HTTPS\n  - number: 80\n    name: http\n    protocol: HTTP\n---\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService # ServiceEntryに対応するIngressを記述\nmetadata:\n  name: gcs-tls\n  namespace: my-app\nspec:\n  hosts:\n  - \"*.googleapis.com\"\n  tls:\n  - match: # 他にもhostがある場合、複数のsni_hostsとdestinationのペアを書く必要があるようだ？\n    - port: 443\n      sni_hosts:\n      - \"storage.googleapis.com\"\n    route:\n    - destination:\n        host: \"storage.googleapis.com\"\n---\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService　\nmetadata:\n  name: metadata-http\n  namespace: my-app\nspec:\n  hosts:\n  - \"metadata.google.internal\"\n  http: # プロトコルがhttpの場合はこちら\n  - route:\n    - destination:\n        host: \"metadata.google.internal\"\n        port:\n          number: 80\n</pre>\n<p>これでようやく、最初の構成のアプリケーションがIstioを導入した状態で動作するようになりました。\nまだまだ初心者ですが、これから知見を貯めていきます。</p>"}